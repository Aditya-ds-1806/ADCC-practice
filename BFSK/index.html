<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/styles/default.min.css">
    <link rel="stylesheet" href="../index.css">
    <title>Binary Frequency Shift Keying(BFSK)</title>
</head>

<body class="container">
    <header class="text-center my-5 py-3">
        <h1 class="display-4">Design of BFSK Receivers and BER Analysis</h1>
        <h1 class="text-secondary">Experiment - 2</h1>
    </header>

    <main>
        <section class="mb-5">
            <h2>1 Aim</h2>
            <p>To design a discrete time BFSK communication receiver, and analyze the BER performance.</p>
        </section>
        <section class="mb-5">
            <h2>2 Theory</h2>
            <div class="text-center">
                <figure class="figure">
                    <img src="./images/constellation.png" class="figure-img img-fluid rounded"
                        alt="constellation for BPSK">
                    <figcaption class="figure-caption text-center">Fig.1 Constellation Map</figcaption>
                </figure>
            </div>
            <h3>2.1 ML Rule</h3>
            Let \(Y\) be the output at the receiver and \(X\) be the message at the transmitter. Let \(N\) be the noise
            introduced
            by the AWGN channel. Let \(E_{b}\) be the bit energy. Let \(B_{k} \in X \) be the transmitted bit and
            \(\hat{B} \in Y=[Y_{1k}, Y_{2k}]\) be the the output at the receiver
            corresponding to \(B_{k}\). Then the ML can be written as:
            $$ \hat{B} =
            \begin{cases}
            1, & \frac{f_{Y}(Y_{k}=y \mid B_{k}=1)}{f_{Y}(Y_{k}=y \mid B_{k}=0)} > 1\\
            0, & \frac{f_{Y}(Y_{k}=y \mid B_{k}=1)}{f_{Y}(Y_{k}=y \mid B_{k}=0)} < 1 \end{cases} $$ $$
                \hat{B}=\begin{cases} 1, & \ln\Big(\frac{f_{Y}(Y_{k}=y \mid B_{k}=1)}{f_{Y}(Y_{k}=y \mid B_{k}=0)}\Big)>
                0\\
                0, & \ln\Big(\frac{f_{Y}(Y_{k}=y \mid B_{k}=1)}{f_{Y}(Y_{k}=y \mid B_{k}=0)}\Big) < 0 \end{cases} $$ $$
                    \hat{B}=\begin{cases} 1, &
                    \ln\Bigg(\frac{\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(Y_{1k}-\sqrt{E_{b}})^2}{2\sigma^2}}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(Y_{2k}-0)^2}{2\sigma^2}}}{\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(Y_{1k}-0)^2}{2\sigma^2}}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(Y_{2k}-\sqrt{E_{b}})^2}{2\sigma^2}}}\Bigg)>
                    0 \\
                    0, &
                    \ln\Bigg(\frac{\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(Y_{1k}-\sqrt{E_{b}})^2}{2\sigma^2}}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(Y_{2k}-0)^2}{2\sigma^2}}}{\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(Y_{1k}-0)^2}{2\sigma^2}}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(Y_{2k}-\sqrt{E_{b}})^2}{2\sigma^2}}}\Bigg)
                    > 0\end{cases} $$
                    $$ \hat{B} =
                    \begin{cases}
                    1, & -2\sqrt{E_{b}}(Y_{1k} - Y_{2k}) > 0\\
                    0, & -2\sqrt{E_{b}}(Y_{1k} - Y_{2k}) < 0 \end{cases} $$ $$ \boxed{\hat{B}=\begin{cases} 1, & Y_{1k}>
                        Y_{2k}\\
                        0, & Y_{1k} < Y_{2k} \end{cases}} $$ <h3>2.2 BER</h3>
                            Let \(E\) be the error bit. Then,
                            $$ P(E=1) = P(\hat{B} \neq B) $$
                            $$ = P(\hat{B}=0 \mid B=1)P(B=1) + P(\hat{B}=1 \mid B=0)P(B=0) $$
                            Since both bits are equally likely:
                            $$ P(B=1) = P(B=0), P(\hat{B}=0 \mid B=1) = P(\hat{B}=1 \mid B=0) $$
                            $$ \Longrightarrow P(E=1) = P(\hat{B}=0 \mid B=1) $$
                            $$ = P(Y_{1k} - Y_{2k} > 0 \mid B=0) $$
                            $$ = P(\sqrt{E_{b}} + N_{1k} - N_{2k} > 0 \mid B=0) $$
                            $$ = P(N_{12} > \sqrt{E_{b}}) $$
                            $$ = P\Bigg(\frac{N_{k}}{\sigma} > \sqrt{\frac{E_{b}}{2\sigma^2}}\Bigg) $$
                            $$ = Q\Bigg(\sqrt{\frac{E_{b}}{2\sigma^2}}\Bigg) $$
                            $$ = Q\Bigg(\sqrt{\frac{E_{b}}{N_{0}}}\Bigg) $$
                            $$ \therefore \boxed{P_{e} = P(E=1) = Q\Bigg(\sqrt{\frac{E_{b}}{N_{0}}}\Bigg)} $$
        </section>
        <section class="mb-5">
            <h2>3 Design</h2>
            <code class="fs-6">
                <pre>
Input: NO_OF_BITS, EB_N0_DB, Bk
Output: BER

Xk = Bk(0 --> [1, 0], 1 --> [0, 1])
ML([bit1, bit2]) = (0 if bit2 - bit1 < 0; 1 if bit2 - bit1 > 0)

for EB_N0 in EB_N0_DB
    Nk = AWGN(EB_N0, 2D)
    Yk = Xk + Nk
    bHat = ML(Yk)
    unchangedBits = count(bHat === Bk)
    BER = 1 - (unchangedBits/NO_OF_BITS)

plot(BER, BER_THEORETICAL)
                </pre>
            </code>
        </section>
        <section id="code" class="mb-5">
            <h2>4 JavaScript Code</h2>
        </section>
        <section class="mb-5">
            <h2>5 Results and Inference</h2>
            <div id="plot"></div>
            <div class="d-flex justify-content-center mt-4">
                <button class="btn btn-primary mx-5" id="simulate">
                    Simulate
                    <span class="spinner-border spinner-border-sm d-none"></span>
                </button>
                <button class="btn btn-primary mx-5" id="simulation" disabled>Save Simulation Data</button>
                <button class="btn btn-primary mx-5" id="theory" disabled>Save Theoretical Data</button>
            </div>
            <div class="row">
                <div class="col-sm-6">
                    <figure class="figure">
                        <img src="./images/10^3.svg" class="figure-img img-fluid rounded">
                        <figcaption class="figure-caption text-center">Fig.2 Simulation result with \(10^3\) bits</figcaption>
                    </figure>
                </div>
                <div class="col-sm-6">
                    <figure class="figure">
                        <img src="./images/10^4.svg" class="figure-img img-fluid rounded">
                        <figcaption class="figure-caption text-center">Fig.3 Simulation result with \(10^4\) bits</figcaption>
                    </figure>
                </div>
                <div class="col-sm-6">
                    <figure class="figure">
                        <img src="./images/10^5.svg" class="figure-img img-fluid rounded">
                        <figcaption class="figure-caption text-center">Fig.4 Simulation result with \(10^5\) bits</figcaption>
                    </figure>
                </div>
                <div class="col-sm-6">
                    <figure class="figure">
                        <img src="./images/10^6.svg" class="figure-img img-fluid rounded">
                        <figcaption class="figure-caption text-center">Fig.5 Simulation result with \(10^6\) bits</figcaption>
                    </figure>
                </div>
            </div>
            <p>
                From the above figures, it can be seen that simulated \(P_{e}\) and theoretical \(P_{e}\) match better with
                increase in the number of bits. This is because, to get confidence in the simulated results, there must be
                sufficient number of bit errors. For example in figure \((5)\), to get a bit error rate of \(10^{-5}\), one needs to
                send at
                least \(10^6\) bits. Similar conclusions can be drawn from the other figures shown above. Also, it can be
                seen that the \(P_{e}\) reduces graudally with increase in \(\frac{E_{b}}{N_{0}}\), the SNR per bit. This
                is due to the fact that as the SNR per bit increases, the signal becomes less affected by the presence of
                noise, thus reducing errors in ML detection. Finally, another important thing to be noted is that the simulated
                curve is not only affected by the symbol errors, but is also affected by floating point errors.
            </p>
        </section>
    </main>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jstat/1.9.5/jstat.min.js"
        integrity="sha512-MGT8BGoc8L3124PwHEGTC+M8Hu9oIbZOg8ENcd92sQKKidWKOOOZ6bqQemqYAX0yXJUnovOkF4Hx9gc/5lVxPw=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>
    <script src="https://cdn.plot.ly/plotly-2.4.2.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/highlight.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="module" src="index.js"></script>
</body>

</html>
